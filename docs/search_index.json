[
["data-check.html", "Chapter 4 Data Check 4.1 Corpus principles 4.2 Corpus properties 4.3 The research of suspect pieces of texts", " Chapter 4 Data Check library(knitr) knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE) library(quanteda) library(dplyr) library(ggplot2) In this chapter, we present some basic test of the data collected. The tools are basesd on the package quanteda. The reader non familiar with this package could have a look at Quick Start available on the website of the project : https://quanteda.io/articles/quickstart.html qd&lt;-readRDS(&quot;_data/qd/en_USA_nytime_20200101_20200630.RDS&quot;) 4.1 Corpus principles A corpus is designed to be a “library” of original documents that have been converted to plain, UTF-8 encoded text, and stored along with meta-data at the corpus level and at the document-level. We have a special name for document-level meta-data: docvars. These are variables or features that describe attributes of each document.A corpus is designed to be a more or less static container of texts with respect to processing and analysis. This means that the texts in corpus are not designed to be changed internally through (for example) cleaning or pre-processing steps, such as stemming or removing punctuation. Rather, texts can be extracted from the corpus as part of processing, and assigned to new objects, but the idea is that the corpus will remain as an original reference copy so that other analyses – for instance those in which stems and punctuation were required, such as analyzing a reading ease index – can be performed on the same corpus. To extract texts from a corpus, we use an extractor, called texts(). texts(qd)[2] ## text2 ## &quot;Rocket Launches, Trips to Mars and More 2020 Space and Astronomy Events&quot; To summarize the texts from a corpus, we can call a summary() method defined for a corpus. summary(qd, n = 5) ## Corpus consisting of 30265 documents, showing 5 documents: ## ## Text Types Tokens Sentences date ## text1 10 10 1 2020-01-01 00:00:08 ## text2 12 13 1 2020-01-01 00:05:07 ## text3 13 14 1 2020-01-01 01:00:05 ## text4 12 13 2 2020-01-01 03:00:11 ## text5 5 5 1 2020-01-01 03:00:13 4.2 Corpus properties We can save the output from the summary command as a data frame in order to obtain for each text the number of sentences and words. tokeninfo &lt;- summary(qd,length(qd)) kable(head(tokeninfo)) Text Types Tokens Sentences date text1 10 10 1 2020-01-01 00:00:08 text2 12 13 1 2020-01-01 00:05:07 text3 13 14 1 2020-01-01 01:00:05 text4 12 13 2 2020-01-01 03:00:11 text5 5 5 1 2020-01-01 03:00:13 text6 9 9 1 2020-01-01 04:22:34 What is the distribution of the number of sentences ? x&lt;-tokeninfo %&gt;% group_by(Sentences) %&gt;% summarise(nb = n()) %&gt;% ungroup() %&gt;% mutate(pct = 100*nb/sum(nb)) kable(x,caption = &quot;Number of sentences of news&quot;, digits=1) Table 4.1: Number of sentences of news Sentences nb pct 1 25933 85.7 2 4021 13.3 3 270 0.9 4 41 0.1 What is the distribution of the number of tokens ? summary(tokeninfo$Tokens) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.00 8.00 11.00 10.73 13.00 66.00 p&lt;-ggplot(tokeninfo,aes(x=Tokens))+geom_histogram() p 4.3 The research of suspect pieces of texts In many case, the news are characterized by the recurrence of pieces of text added by the journal that are not part of the text itself like Read More, See also. A good way to discover these supsect pieces of texts is to compute the most frequent n-grams # bigrams qd %&gt;% tokens() %&gt;% tokens_ngrams(n=2L) %&gt;% dfm() %&gt;% topfeatures(50) ## new_york the_new york_times -_the the_coronavirus ## 4623 4234 4147 4093 899 ## u.s_. of_the in_the ,_dies dies_at ## 884 751 653 601 562 ## :_&#39; how_to &#39;_: ,_&#39; live_updates ## 509 456 443 428 406 ## opinion_| review_: ,_a on_the ,_and ## 383 367 347 346 340 ## ,_but in_a to_the :_a coronavirus_, ## 335 330 310 308 306 ## &#39;_the coronavirus_in n.y.c_. :_the &#39;_review ## 288 285 277 272 272 ## :_live :_what day_: the_day for_the ## 266 254 248 245 243 ## &#39;_and and_the ._&#39; updates_: :_your ## 242 235 233 226 221 ## ,_the a_new in_new ._: ._- ## 210 210 208 207 200 ## what_to and_&#39; from_the ,_2020 to_be ## 198 197 191 190 189 # trigrams qd %&gt;% tokens() %&gt;% tokens_ngrams(n=3L) %&gt;% dfm() %&gt;% topfeatures(50) ## the_new_york new_york_times -_the_new ## 4162 4147 4069 ## ,_dies_at &#39;_review_: of_the_day ## 521 272 230 ## the_day_: :_live_updates ._-_the ## 225 212 200 ## live_updates_: coronavirus_-_the in_n.y.c_. ## 177 176 174 ## the_u.s_. &#39;_and_&#39; coronavirus_live_updates ## 165 165 163 ## in_new_york what&#39;s_on_tv quotation_of_the ## 155 152 147 ## ?_-_the &#39;_-_the in_n.y_. ## 139 107 103 ## is_dead_at ,_is_dead new_york_city ## 100 98 92 ## word_+_quiz +_quiz_: :_what_happened ## 85 85 85 ## during_the_coronavirus briefing_:_what n.y.c_._: ## 84 83 82 ## coronavirus_briefing_: the_coronavirus_pandemic what_happened_today ## 82 81 81 ## lesson_of_the day_:_&#39; review_:_a ## 78 78 76 ## ._&#39;_s n.y_._: coronavirus_in_n.y.c ## 75 75 72 ## on_politics_: n.y.c_._this coronavirus_in_n.y ## 70 68 68 ## ._this_weekend :_latest_updates of_the_coronavirus ## 66 64 63 ## in_a_pandemic in_the_u.s opinion_|_the ## 63 63 61 ## your_monday_briefing ._:_latest ## 61 60 In our example, we can eliminate some obvious problems (- The New York Times, Live updates) and correct some abbreviations (U.S., N.Y.C., N.Y.) which are likely to create problems. qd2&lt;-qd texts(qd2)&lt;-gsub(pattern = &quot; - the new york times&quot;, x = texts(qd2),replacement = &quot;&quot;, ignore.case=TRUE) texts(qd2)&lt;-gsub(pattern = &quot;live updates&quot;, x = texts(qd2),replacement = &quot;&quot;, ignore.case=TRUE) texts(qd2)&lt;-gsub(pattern = &quot;u.s.&quot;, x = texts(qd2),replacement = &quot;US&quot;, ignore.case=TRUE) texts(qd2)&lt;-gsub(pattern = &quot;n.y.c.&quot;, x = texts(qd2),replacement = &quot;NYC&quot;, ignore.case=TRUE) texts(qd2)&lt;-gsub(pattern = &quot;n.y.&quot;, x = texts(qd2),replacement = &quot;NYC&quot;, ignore.case=TRUE) qd2 %&gt;% tokens() %&gt;% tokens_ngrams(n=2L) %&gt;% dfm() %&gt;% topfeatures(50) ## the_coronavirus of_the in_the ,_dies dies_at ## 899 751 646 599 562 ## new_york :_&#39; how_to &#39;_: ,_&#39; ## 552 509 456 440 425 ## opinion_| review_: on_the ,_a ,_and ## 383 367 344 343 340 ## ,_but in_a coronavirus_, to_the :_a ## 329 326 305 304 303 ## coronavirus_: &#39;_the coronavirus_in in_nyc :_the ## 296 288 285 276 271 ## &#39;_review :_what day_: the_day for_the ## 266 253 248 244 242 ## &#39;_and and_the :_your ._&#39; a_new ## 238 235 218 212 210 ## ,_the in_new and_&#39; what_to from_the ## 208 205 197 197 191 ## ,_2020 the_pandemic to_be bernie_sanders of_coronavirus ## 190 187 187 184 181 ## a_pandemic ,_is is_a for_a the_us ## 177 176 172 167 166 saveRDS(qd2,&quot;_data/qd/en_USA_nytime_20200101_20200630_clean.RDS&quot;) "]
]
