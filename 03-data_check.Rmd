# Data Check and Clean {#data_check}

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(quanteda)
library(dplyr)
library(ggplot2)
library(tidytext)

```




In this chapter, we describe the different operation of checking and cleaning that should be necessarily realized before any sound exploration of corpus. The majority of the procedures described here can not be done automatically and implies a human expertise at each step in order to control the results and propose remediations in case of problems.


## Prerequites : quanteda (or tidytext)


The operations will be realized jointly with the two packages **quanteda** and **tidytext**. In the majority of case, we will use quanteda but in some specific situations tidytext will appear more adapted and will be used, thanks to (relatively) easy possibilities of exchange between the two format of dta storage of corpora. 

- The reader non familiar with quanteda  should have a look at *Quick Start* available on the website of the project :

https://quanteda.io/articles/quickstart.html

- The reader non familiar with tidytext will find detailed explnation in the following book : 


https://www.tidytextmining.com/


We start by loading the corpus of news from *The Guardian (UK)* that has been transformed into quanteda corpus in the previous chapter (**ref**). we also transform the time variable in date and sort the news by historical order.

```{r}
qd<-readRDS("_data/qd/en_GBR_guardi.Rdata")
qd$date<-as.Date(qd$date)
qd<-qd[order(qd$date)]
str(qd)
```
This structure of data appears a bit tricky at first glance (especially for old user of quanteda that has experimented modification between version 1 and 2 ...) but it offers very good guaranties of quality and efficiency. As explained by the author of the quanteda package :

>A corpus is designed to be a "library" of original documents that have been converted to plain, UTF-8 encoded text, and stored along with meta-data at the corpus level and at the document-level.  We have a special name for document-level meta-data: *docvars*.  These are variables or features that describe attributes of each document.A corpus is designed to be a more or less static container of texts with respect to processing and analysis.  This means that the texts in corpus are not designed to be changed internally through (for example) cleaning or pre-processing steps, such as stemming or removing punctuation.  Rather, texts can be extracted from the corpus as part of processing, and assigned to new objects, but the idea is that the corpus will remain as an original reference copy so that other analyses -- for instance those in which stems and punctuation were required, such as analyzing a reading ease index -- can be performed on the same corpus.

With the new data structure it is for example easy to extract a specific text from the corpus with an extractor, called `texts()`.  We can also have an easy access to information associated to each document like the date in our example. For example the news 666 is defined by :

```{r}
texts(qd)[666]
qd$date[666]
```
For those who really don't want to use quanteda, it is also simple to convert it to other format and in particular a simple data.frame where we can find only the id of document, the text and the document level information. But we have lost all the other information like the document metadata.

```{r}
df<-convert(qd, to="data.frame")
head(df)
```

## CHECK 1 : length of texts

We can use the summary command applied to the whole corpus in order to obtain for each text the number of sentences and words. This operation can take some time because quanteda will proceed to a tokenization of the corpus. We suggest to store this information in the quanteda object.


```{r}
# Compute number of sentences and tokens
tokeninfo <- summary(qd,length(qd))

#store in quanteda object
docvars(qd, field="Tokens")<-tokeninfo$Tokens
docvars(qd, field="Sentences")<-tokeninfo$Sentences

# Visualize
head(docvars(qd))
```

### Number of sentences

We examine the number of sentences of texts

```{r}
nsent<-qd$Sentences
summary(nsent)
p<-ggplot(docvars(qd),aes(x=Sentences))+geom_histogram()
p

```

The distribution appears strongly asymmetric with a majority of news with 3 to 7 sentences but a minoity with much longer size. We can zoom on the news with less than 20 sentences for a better view of the distribution :

```{r}
qd2<-qd[qd$Sentences<20]
p<-ggplot(docvars(qd2),aes(x=Sentences))+geom_bar(stat="count",)
p

```


At this point, it could be useful to proceed to an harmonisation of the length of texts with two possibles options :

- delete the news with too much sentences
- keep only the k first sentences of each text with the *corpus_reshape* function. 

But we do not want here to anaticipate on what will decide the user so we let the issue open. 


### Number of tokens

```{r}
nbtoks<-qd$Tokens
summary(nbtoks)
p<-ggplot(docvars(qd),aes(x=Tokens))+geom_histogram()
p
```
We observe the same asymmetry for the number of tokens. In the ajority of case, the number of tokens is comprise between 100 and 180 Tokens but the minimum is 12 and the maximum 3362. 

```{r}
p<-ggplot(docvars(qd2),aes(x=Tokens))+geom_histogram(stat_bin=25)+scale_x_continuous(lim = c(0,300))
p
```



## CHECK 2 : Time distribution 

The second step is to verify the stability of the news through times, in terms of frequencies. The main danger here is the risk to observe discontinuities in the distribution  which could induce biases in the comparison of evolution through time.


### News without date

After a check of the time distribution of news, we eliminate 22 news without date

```{r}
summary(docvars(qd,"date"))
qd<-qd[is.na(qd$date)==F]
```



### Distribution of news through time


We introduce a function when_count for an easier extraction of time series with different levels of time span. The interest of this function is to add zero value for time period where no news has been observed, which would not be the case with a simple table function. 

```{r}
#### ---------------- when_count -----------------
#' @title identify number of news per time period
#' @name when_count
#' @description count the numbers of news per time period and add zero for missing period
#' @param corpus a corpus of news (in qd format) with a column "time"
#' @param span a time period of aggregation (days, weeks, months, year)



when_count<-function(corpus = qd, span = 'weeks' )
{
 # corpus<-qd
#  span<-"weeks"

  
  time<-docvars(corpus,"date")
  tab<-tibble::tibble(date=as.Date(time))
  item_date <- tab %>%
    group_by(date = cut(date, span, start.on.monday = TRUE)) %>%
    summarise(nb = n())
  item_date$date <- as.Date(item_date$date)
  all_date<-data_frame(date=seq(min(as.Date(item_date$date)),max(as.Date(item_date$date)),span))
  tem_date<-left_join(all_date,item_date)
  item_date$nb[is.na(item_date$nb)]<-0
  
  return(item_date)
}

```

#### daily distribtion

```{r, news_by_day}
news_by_day<-when_count(qd,span="days")
p<-ggplot(news_by_day, aes(x=date, y=nb)) + geom_line()+ scale_y_continuous(limits=c(0,max(news_by_day$nb)))
p
```
We discover here that, despite the fact that the first news is observed the 29 April 2013, it is only after the 15th August 2016 that the flow of news become regular with an average level of 100 news per day. Therefore we reduce the sample to this period :
```{r, news_by_day_2}

qd<-corpus_subset(qd,qd$date>as.Date("2016-08-14"))
news_by_day<-when_count(qd,span="days")
p<-ggplot(news_by_day, aes(x=date, y=nb)) + geom_line()+ scale_y_continuous(limits=c(0,max(news_by_day$nb)))
p
```
For the new period, we can say that the flow is relatively regular, with very few discontinuities corresponding to zero news during one or several days.  

#### weekly distribtion

```{r, news_by_week}
news_by_week<-when_count(qd,span="weeks")
p<-ggplot(news_by_week, aes(x=date, y=nb)) + geom_line() + scale_y_continuous(limits=c(0,max(news_by_week$nb)))
p
```
The analysis by week reveals a clear perturabation during the weeks of 23 March 2019 and 3 February 2020. The other decline observed at the end of the years 2016, 2018, 2019 and 2020 are not related to problems of data collection but rather to a reduction of the activity of journalist between X'mas and New Year.Surprisingly, this decline is not observed at the end of the year 2017. The final decline is related to the fact that the final week starting the 29 June 2020 is not complete because colection stopped the 30 june 2020.


## CHECK 3 : Suspect words

A classical problem with the news collected from RSS is the existence of sequence of words that are not related to news itself but to newspaper structure. For example, the systematic addition of a copyright (*(c) the Journal*) or the introduction of references (*See also*) and many other things that can be considered as external to the message provided by the news itself. Here they are no miracle solution that can produce an automatic correction. But some technics can help the analyst to simplify the task of correction.


### Tips 1 : simple word fequencies

A simple analysis of most frequent tokens can help to discover suspect words. 


```{r}
toks<-tokens(qd, remove_numbers = TRUE, remove_punct = TRUE, remove_separators =TRUE)
dfm <- dfm(toks, remove = stopwords("english"))
top10<-textstat_frequency(dfm,n=10)
kable(top10)

```

The simple examination of the top 10 reveals the existence of two suspect words *continue* and *reading*.  They can be explained by the existence of the string *Continue Reading* at the end of the majority of news. For example :


```{r}
texts(qd)[24]
```


### Tips 2 : most frequent n-grams

Another approach consist in the extraction of most frequent n-grams i.e. couples of words frequently associated. This can be done with various value of n. But it is generally sufficient to check for n= 2, 3 or 4.*

```{r}
bigram<-tokens_ngrams(toks,n=c(2))
dfm2<-dfm(bigram)
top10<-textstat_frequency(dfm2,n=10)
kable(top10)
```

Here, the conclusion is clear concerning our usual suspect. We can therefore proceed to the elimination of the problem with a regular expression

```{r}
texts(qd)<-gsub("continue reading...", "", texts(qd))
texts(qd)<-gsub("continue reading", "", texts(qd))
```


## CHECK 4 : Human expertise

All the previous check can not replace human expert. Therefore, we strongly suggest to examine a random sample of texts extract from the news in order to have an empirical view on the texts that will be further subject to time consuming analysis. It is certainly not a vaste of time to discover issues early and avoid to replicate the whole work (pity for the climate !).

A random sample of news can be easily extracted with corpus_sample function. Let's take a sample of 10 here (but 100 would be certainly better)

```{r}
samp<-corpus_sample(qd,size=10)
texts(samp)
```

The sample appears correct but some corrections could be added. We could for example eliminate the double quotes that will complicate the analysis of text. We could also decide to eliminate the final sentences that are not completed, etc. But it can also be important to keep the text as similar as possible to the initial one and to delay these final check to the next chapter where the data will be focused on a specific question.

The last step here is to store the clean file for further use. Here we use the initial name but with adding information on the period of coverage. 


```{r}
saveRDS(qd, "_data/qd/en_GBR_guardi_20160815_20200630.Rdata")
```








